{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-synthetic 20newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we run NMF, LDA, NCPD, and Online NCPD as dynamic topic modeling techniques for the semi-synthetic 20newsgroups dataset described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4erDBMWTyQ_D",
    "outputId": "7423bfa8-d24d-43a7-d03d-dbf563a85e9a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import tensorly as tl\n",
    "import nltk\n",
    "import gensim\n",
    "import random\n",
    "import collections \n",
    "from gensim import corpora,models\n",
    "from nltk.corpus import stopwords\n",
    "from tensorly import fold\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorly.decomposition import non_negative_parafac\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from config import data_dir, results_dir\n",
    "from covid19 import plotting, utils\n",
    "from covid19.online_CPDL.ocpdl import Online_CPDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "rank = 5\n",
    "n_top_words = 5 # number of keywords to display for each topic\n",
    "save_figures = True\n",
    "load_NMF_factors = False # True for loading pre-saved NMF factors; False for computing the factors\n",
    "load_NCPD_factors = False # True for loading pre-saved NCPD factors; False for computing the factors\n",
    "load_lda_model = False # True for loading trained LDA model; False for training a model\n",
    "load_ONCPD_factors = False # True for loading pre-saved ONCPD factors; False for computing the factors\n",
    "local_path = results_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  20newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the data with the selected categories\n",
    "remove_tuple = ('headers', 'footers', 'quotes') # remove headers, footers, and quotes from all documents\n",
    "newsgroups_subset_atheism = fetch_20newsgroups(subset='all', categories=['alt.atheism'], shuffle=True, random_state = 1, \n",
    "                                               remove=remove_tuple)\n",
    "newsgroups_subset_space = fetch_20newsgroups(subset='all', categories=['sci.space'], shuffle=True, random_state = 1,\n",
    "                                               remove=remove_tuple)\n",
    "newsgroups_subset_forsale = fetch_20newsgroups(subset='all', categories= ['misc.forsale'], shuffle=True, random_state = 1,\n",
    "                                               remove=remove_tuple)\n",
    "newsgroups_subset_baseball = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball'], shuffle=True, random_state =1,\n",
    "                                               remove=remove_tuple)\n",
    "newsgroups_subset_windowsx = fetch_20newsgroups(subset='all', categories=['comp.windows.x'], shuffle=True, random_state = 1,\n",
    "                                               remove=remove_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum number of documents for each category\n",
    "min_doc = np.min([len(newsgroups_subset_atheism.data),\n",
    "                  len(newsgroups_subset_space.data), \n",
    "                  len(newsgroups_subset_forsale.data), \n",
    "                  len(newsgroups_subset_baseball.data),\n",
    "                  len(newsgroups_subset_windowsx.data)])\n",
    "print(min_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_slice_lock = 26 # number of documents per slice\n",
    "\n",
    "numb_A = 2 #number of time slices from aethism category\n",
    "numb_S = 18 #number of time slices from space category\n",
    "numb_B_1 = 3 #number of time slices from baseball category for the first instance\n",
    "numb_F = 12 #number of time slices from sale category\n",
    "numb_W = 2 #number of time slices from Windows X category\n",
    "numb_B_2 = 3 #number of time slices from baseball category for the second instance\n",
    "\n",
    "numb_B = numb_B_1 + numb_B_2\n",
    "\n",
    "num_time_slices = numb_A + numb_B + numb_S + numb_F + numb_W\n",
    "print(num_time_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample each category based on the outline above\n",
    "data_samples_aethism = newsgroups_subset_atheism.data[:numb_A*doc_slice_lock]\n",
    "data_samples_space = newsgroups_subset_space.data[:numb_S*doc_slice_lock]\n",
    "data_samples_forsale = newsgroups_subset_forsale.data[:numb_F*doc_slice_lock]\n",
    "data_samples_baseball = newsgroups_subset_baseball.data[:numb_B*doc_slice_lock]\n",
    "data_samples_windowsx = newsgroups_subset_windowsx.data[:numb_W*doc_slice_lock]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Corpus from all categories in order atheism, space, forsale, baseball\n",
    "newsgroups_subset_cats = data_samples_aethism + data_samples_space + data_samples_forsale + data_samples_baseball + data_samples_windowsx\n",
    "print(f\"Total number of documents in all categories is {len(newsgroups_subset_cats)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract TF-IDF weights and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the choices of the parameters of TfidfVectorizer\n",
    "n_features = 5000\n",
    "max_df=0.95\n",
    "stop_words_list = nltk.corpus.stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(max_df=max_df, max_features=n_features,\n",
    "                            stop_words=stop_words_list)\n",
    "vectors = vectorizer.fit_transform(newsgroups_subset_cats) \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = np.array(dense).transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all features/words extracted\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the dataset into a dynamic tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features matrix shape\n",
    "print(\"Shape of feature matrix {}.\".format(denselist.shape))\n",
    "print(\"Dimensions: (vocabulary, headlines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate each category\n",
    "X_aethism = denselist[:,0:numb_A*doc_slice_lock]\n",
    "print(X_aethism.shape)\n",
    "\n",
    "X_space = denselist[:,numb_A*doc_slice_lock:(numb_A+numb_S)*doc_slice_lock]\n",
    "print(X_space.shape)\n",
    "\n",
    "X_forsale = denselist[:,(numb_A+numb_S)*doc_slice_lock:(numb_A+numb_S+numb_F)*doc_slice_lock]\n",
    "print(X_forsale.shape)\n",
    "\n",
    "X_baseball = denselist[:,(numb_A+numb_S+numb_F)*doc_slice_lock:(numb_A+numb_S+numb_F+numb_B)*doc_slice_lock]\n",
    "print(X_baseball.shape)\n",
    "\n",
    "X_windowsx = denselist[:,(numb_A+numb_S+numb_F+numb_B)*doc_slice_lock:(numb_A+numb_S+numb_F+numb_B+numb_W)*doc_slice_lock]\n",
    "print(X_windowsx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the splitting was done properly\n",
    "np.all(np.concatenate((X_aethism, X_space, X_forsale, X_baseball, X_windowsx), axis = 1) == denselist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize each category into a tensor of dimensions: (documents, vocabulary, time)\n",
    "X_aethism_tens = fold(X_aethism, 1, (X_aethism.shape[1] // numb_A, X_aethism.shape[0], numb_A))\n",
    "X_aethism_tens = X_aethism_tens[:, :, 0:numb_A]\n",
    "print(X_aethism_tens.shape)\n",
    "\n",
    "X_space_tens = fold(X_space, 1, (X_space.shape[1] // numb_S, X_space.shape[0], numb_S))\n",
    "X_space_tens = X_space_tens[:, :, 0:numb_S]\n",
    "print(X_space_tens.shape)\n",
    "\n",
    "X_baseball_tens = fold(X_baseball, 1, (X_baseball.shape[1] // numb_B, X_baseball.shape[0], numb_B))\n",
    "X_baseball_tens = X_baseball_tens[:, :, 0:numb_B_1]\n",
    "print(X_baseball_tens.shape)\n",
    "\n",
    "X_forsale_tens = fold(X_forsale, 1, (X_forsale.shape[1] // numb_F, X_forsale.shape[0], numb_F))\n",
    "X_forsale_tens = X_forsale_tens[:, :, 0:numb_F]\n",
    "print(X_forsale_tens.shape)\n",
    "\n",
    "X_windowsx_tens = fold(X_windowsx, 1, (X_windowsx.shape[1] // numb_W, X_windowsx.shape[0], numb_W))\n",
    "X_windowsx_tens = X_windowsx_tens[:, :, 0:numb_W]\n",
    "print(X_windowsx_tens.shape)\n",
    "\n",
    "X_baseball_tens_2 = fold(X_baseball, 1, (X_baseball.shape[1] // numb_B, X_baseball.shape[0], numb_B))\n",
    "X_baseball_tens_2 = X_baseball_tens_2[:, :, numb_B_1:numb_B_1+numb_B_2]\n",
    "print(X_baseball_tens_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Dataset\n",
    "tensor_20news = np.concatenate((X_aethism_tens, X_space_tens, X_baseball_tens, X_forsale_tens, X_windowsx_tens, X_baseball_tens_2) , axis = 2).T\n",
    "print(\"Shape of the tensor {}.\".format(tensor_20news.shape)) \n",
    "print(\"Dimensions: (time, vocabulary, documents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap Visualization\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5, context=\"talk\")\n",
    "\n",
    "def heatmap(\n",
    "    data,\n",
    "    x_tick_labels=None,\n",
    "    x_label=\"\",\n",
    "    y_tick_labels=None,\n",
    "    y_label=\"\",\n",
    "    figsize=(7, 9),\n",
    "    max_data=None,\n",
    "):\n",
    "    \"\"\"Plot heatmap.\n",
    "    Args:\n",
    "        data: (2d array) data to be plotted (topics x date)\n",
    "        x_tick_labels (list of str)\n",
    "    Returns:\n",
    "        fig\n",
    "        ax\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(\n",
    "        data,\n",
    "        rasterized=True,\n",
    "        vmax=max_data,\n",
    "        cbar_kws=dict(use_gridspec=False, location=\"top\"),\n",
    "    )\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(np.arange(0, data.shape[0], 1.0) + 0.5, rotation=0)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    if y_tick_labels is None:\n",
    "        y_tick_labels = [topic_num + 1 for topic_num in range(data.shape[0])]\n",
    "    ax.set_yticklabels(y_tick_labels)\n",
    "\n",
    "    if x_tick_labels is not None:\n",
    "        labels = [x_tick_labels[int(item.get_text())] for item in ax.get_xticklabels()]\n",
    "        ax.set_xticklabels(labels)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organized matrix dataset\n",
    "matrix_20news = np.concatenate(tensor_20news, 1)\n",
    "print(\"Shape of matrix {}.\".format(matrix_20news.shape))\n",
    "print(\"Dimensions: (vocabulary, documents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "if load_NMF_factors:\n",
    "    W, H = pickle.load(open(os.path.join(local_path,\"20news_NMF_factors.pickle\"), \"rb\"))\n",
    "else:\n",
    "    nmf = NMF(n_components=rank, init='nndsvd', max_iter = 400)\n",
    "    W = nmf.fit_transform(matrix_20news) # Dictionary\n",
    "    H = nmf.components_ # Topic representations\n",
    "    NMF_factors = W,H\n",
    "    with open(os.path.join(local_path,\"20news_NMF_factors.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(NMF_factors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display topics \n",
    "for topic_idx, topic in enumerate(W.T):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for each time slice\n",
    "topics_over_time = np.split(H, num_time_slices, axis=1)\n",
    "\n",
    "# Average topic distributions over time.\n",
    "avg_topics_over_time = [np.mean(topics, axis=1) for topics in topics_over_time]\n",
    "\n",
    "# Normalize to get the distribution over topics for each day.\n",
    "avg_topics_over_time = np.array(\n",
    "    [topics / np.sum(topics) for topics in avg_topics_over_time]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualize topic distributions\n",
    "date_strs = [str(x+1) for x in range(tensor_20news.shape[0])]\n",
    "\n",
    "y_tick_labels = [\"{}\".format(i + 1) for i in range(rank)]\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    avg_topics_over_time.T,\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Time\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"Topic\",\n",
    "    figsize = (6,4),\n",
    ")\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"NMF_Normalized_Topic_Time_20news.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_NCPD_factors:\n",
    "    factors = pickle.load(open(os.path.join(local_path,\"20news_NCPD_factors.pickle\"), \"rb\"))\n",
    "    weights = pickle.load(open(os.path.join(local_path,\"20news_NCPD_weights.pickle\"), \"rb\"))\n",
    "else:\n",
    "    factors = non_negative_parafac(tensor_20news, rank=rank, init = 'svd') \n",
    "    if len(factors) == 2:\n",
    "        weights, factors = factors\n",
    "        print(weights)\n",
    "        with open(os.path.join(local_path,\"20news_NCPD_weights.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(weights, f)\n",
    "    with open(os.path.join(local_path,\"20news_NCPD_factors.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(factors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the shape of the factor matrices\n",
    "for i in range(len(factors)):\n",
    "    print(factors[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "for topic_idx, topic in enumerate(factors[1].T):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting normalized temporal factor\n",
    "time_norm = factors[0]/factors[0].sum(axis=1)[:,None] # row sum equal to 1\n",
    "y_tick_labels = [\"{}\".format(i + 1) for i in range(rank)]\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    time_norm.T, #factors[2].T for un-normalized factor\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Time\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"\",\n",
    "    figsize = (6,4),\n",
    ")\n",
    "\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"NCPD_Normalized_Topic_Time_20news.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LDA + Temporal evolution extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured corpus based on constructed tensor\n",
    "corpus = data_samples_aethism + data_samples_space + data_samples_baseball[0:numb_B_1*doc_slice_lock] + \\\n",
    "            data_samples_forsale + data_samples_windowsx + \\\n",
    "            data_samples_baseball[numb_B_1*doc_slice_lock:(numb_B_1+numb_B_2)*doc_slice_lock]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "tokenized_docs = list(map(vectorizer.build_analyzer(),corpus)) # pre-process and tokenize documents using tfidfvectorizer\n",
    "# Filter processed_docs based on features list\n",
    "processed_docs = []\n",
    "for doc in tokenized_docs:\n",
    "    processed_docs.append([wd_str for wd_str in doc if(wd_str in feature_names)])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs) # define a (gensim) dictionary\n",
    "print(len(dictionary))\n",
    "\n",
    "# Create bag-of-words corpus (gensim format)\n",
    "corpus=[dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Preprocessing (to create a separate dictionary from NMF/NCPD)\n",
    "\"\"\"\n",
    "# Data Preprocessing\n",
    "processed_docs = list(map(vectorizer.build_analyzer(),corpus)) # pre-process and tokenize documents using tfidfvectorizer\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs) # define a (gensim) dictionary\n",
    "dictionary.filter_extremes(no_below=min_df,no_above=max_df,keep_n=n_features) # use the same parameters as tfidfvectorizer\n",
    "print(len(dictionary))\n",
    "\n",
    "# Create bag-of-words corpus (gensim format)\n",
    "corpus=[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if dictionaries are the same\n",
    "collections.Counter(list(dictionary.values())) == collections.Counter(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "run_LDA_tfidf = False\n",
    "\n",
    "if run_LDA_tfidf:\n",
    "    # Convert BOW Corpus to TFIDF Corpus\n",
    "    tfidf=models.TfidfModel(corpus)\n",
    "    corpus=tfidf[corpus]\n",
    "\n",
    "if load_lda_model:\n",
    "    lda_model = pickle.load(open(os.path.join(local_path,\"20news_lda_model.pickle\"), \"rb\"))\n",
    "else:\n",
    "    lda_model = gensim.models.LdaModel(corpus,\n",
    "                                    num_topics=rank, \n",
    "                                    id2word = dictionary, \n",
    "                                    passes = 20,\n",
    "                                    minimum_probability = 0,\n",
    "                                    random_state = 13\n",
    "                                      )\n",
    "   \n",
    "    with open(os.path.join(local_path,\"20news_lda_model.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(lda_model, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "for topic_idx, topic in enumerate(lda_model.get_topics()):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([dictionary[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA representation matrix (topics by documents)\n",
    "lda_rep_mat = np.empty((rank, len(corpus)))\n",
    "for numb in range(len(corpus)):\n",
    "    for index, score in lda_model.get_document_topics(corpus[numb], minimum_probability = 0):\n",
    "        lda_rep_mat[index, numb] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distributions for each time slice\n",
    "lda_topics_over_time = np.split(lda_rep_mat, num_time_slices, axis=1)\n",
    "\n",
    "# Average topic distributions over time.\n",
    "lda_avg_topics_over_time = [np.mean(topics, axis=1) for topics in lda_topics_over_time]\n",
    "\n",
    "# Normalize to get the distribution over topics for each day.\n",
    "lda_avg_topics_over_time = np.array(\n",
    "    [topics / np.sum(topics) for topics in lda_avg_topics_over_time]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize topic distributions\n",
    "y_tick_labels = [\"{}\".format(i + 1) for i in range(rank)]\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    lda_avg_topics_over_time.T,\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Time\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"\",\n",
    "    figsize = (6,4),\n",
    ")\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"LDA_Normalized_Topic_Time_20news.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online NCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD intialization (Optional)\n",
    "weights, factors = non_negative_parafac(tensor_20news, rank=rank, init = 'svd', n_iter_max=1) \n",
    "print(factors[0].shape)\n",
    "print(factors[1].shape)\n",
    "loading = {}\n",
    "loading.update({'U0':factors[0]})\n",
    "loading.update({'U1':factors[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Online NCPD\n",
    "if load_ONCPD_factors:\n",
    "    factors = pickle.load(open(os.path.join(local_path,\"20news_ONCPD_factors.pickle\"), \"rb\"))\n",
    "else:\n",
    "    OCPDL = Online_CPDL(X=tensor_20news,\n",
    "                        batch_size=tensor_20news.shape[-1]//10,\n",
    "                        iterations=300,\n",
    "                        n_components=rank,\n",
    "                        ini_loading=loading,\n",
    "                        ini_A=None,\n",
    "                        ini_B=None,\n",
    "                        alpha=0,\n",
    "                        beta=1,\n",
    "                        subsample=True)\n",
    "    \n",
    "    result_dict = OCPDL.train_dict(if_compute_recons_error=False,\n",
    "                                   save_folder=None,\n",
    "                                   output_results=True)\n",
    "    loading = result_dict.get(\"loading\")\n",
    "    factors = []\n",
    "    for i in loading.keys():\n",
    "        factors.append(loading.get(str(i)))\n",
    "    \n",
    "    print('!!! X.shape', tensor_20news.shape)\n",
    "    with open(os.path.join(local_path,\"20news_ONCPD_factors.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(factors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the shape of the factor matrices\n",
    "for i in range(len(factors)):\n",
    "    print(factors[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display topics\n",
    "for topic_idx, topic in enumerate(factors[1].T):\n",
    "    message = \"Topic %d: \" % (topic_idx+1)\n",
    "    message += \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize topic distributions\n",
    "date_strs = [str(x+1) for x in range(tensor_20news.shape[0])]\n",
    "\n",
    "time_norm = factors[0]/factors[0].sum(axis=1)[:,None] # row sum equal to 1\n",
    "y_tick_labels = [\"{}\".format(i + 1) for i in range(rank)]\n",
    "\n",
    "fig, ax = heatmap(\n",
    "    time_norm.T, #factors[2].T for un-normalized factor\n",
    "    x_tick_labels=date_strs,\n",
    "    x_label=\"Time\",\n",
    "    y_tick_labels=y_tick_labels,\n",
    "    y_label=\"\",\n",
    "    figsize = (6,4),\n",
    ")\n",
    "\n",
    "\n",
    "if save_figures:\n",
    "    plt.savefig(os.path.join(local_path,\"ONCPD_Normalized_Topic_Time_20news.png\"), bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "20news_new_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
